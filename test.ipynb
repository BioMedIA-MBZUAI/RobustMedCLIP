{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3decad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1791107/4202713513.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"outputs/exp-rank-8/vit/fewshot_10_percent/checkpoints/best_model/model.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_vit.lora_vit.trunk.cls_token\n",
      "lora_vit.lora_vit.trunk.pos_embed\n",
      "lora_vit.lora_vit.trunk.patch_embed.proj.weight\n",
      "lora_vit.lora_vit.trunk.patch_embed.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.0.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.0.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.0.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.0.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.0.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.1.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.1.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.1.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.1.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.1.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.2.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.2.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.2.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.2.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.2.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.3.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.3.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.3.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.3.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.3.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.4.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.4.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.4.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.4.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.4.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.5.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.5.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.5.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.5.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.5.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.6.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.6.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.6.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.6.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.6.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.7.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.7.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.7.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.7.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.7.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.8.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.8.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.8.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.8.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.8.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.9.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.9.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.9.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.9.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.9.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.10.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.10.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.10.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.10.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.10.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.11.norm1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.norm1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.qkv.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.qkv.bias\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.linear_a_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.linear_b_q.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.linear_a_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.linear_b_k.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.linear_a_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.qkv.linear_b_v.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.proj.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.attn.proj.bias\n",
      "lora_vit.lora_vit.trunk.blocks.11.norm2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.norm2.bias\n",
      "lora_vit.lora_vit.trunk.blocks.11.mlp.fc1.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.mlp.fc1.bias\n",
      "lora_vit.lora_vit.trunk.blocks.11.mlp.fc2.weight\n",
      "lora_vit.lora_vit.trunk.blocks.11.mlp.fc2.bias\n",
      "lora_vit.lora_vit.trunk.norm.weight\n",
      "lora_vit.lora_vit.trunk.norm.bias\n",
      "lora_vit.lora_vit.head.proj.weight\n",
      "text_encoder.transformer.embeddings.word_embeddings.weight\n",
      "text_encoder.transformer.embeddings.position_embeddings.weight\n",
      "text_encoder.transformer.embeddings.token_type_embeddings.weight\n",
      "text_encoder.transformer.embeddings.LayerNorm.weight\n",
      "text_encoder.transformer.embeddings.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.0.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.0.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.0.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.0.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.0.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.0.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.0.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.0.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.0.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.0.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.0.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.0.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.0.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.0.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.1.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.1.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.1.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.1.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.1.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.1.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.1.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.1.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.1.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.1.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.1.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.1.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.1.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.1.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.2.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.2.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.2.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.2.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.2.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.2.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.2.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.2.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.2.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.2.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.2.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.2.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.2.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.2.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.3.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.3.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.3.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.3.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.3.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.3.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.3.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.3.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.3.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.3.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.3.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.3.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.3.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.3.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.4.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.4.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.4.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.4.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.4.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.4.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.4.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.4.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.4.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.4.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.4.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.4.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.4.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.4.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.5.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.5.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.5.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.5.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.5.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.5.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.5.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.5.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.5.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.5.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.5.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.5.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.5.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.5.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.6.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.6.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.6.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.6.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.6.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.6.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.6.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.6.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.6.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.6.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.6.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.6.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.6.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.6.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.7.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.7.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.7.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.7.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.7.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.7.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.7.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.7.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.7.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.7.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.7.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.7.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.7.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.7.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.8.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.8.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.8.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.8.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.8.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.8.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.8.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.8.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.8.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.8.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.8.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.8.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.8.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.8.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.9.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.9.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.9.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.9.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.9.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.9.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.9.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.9.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.9.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.9.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.9.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.9.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.9.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.9.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.10.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.10.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.10.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.10.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.10.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.10.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.10.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.10.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.10.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.10.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.10.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.10.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.10.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.10.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.11.attention.self.query.weight\n",
      "text_encoder.transformer.encoder.layer.11.attention.self.query.bias\n",
      "text_encoder.transformer.encoder.layer.11.attention.self.key.weight\n",
      "text_encoder.transformer.encoder.layer.11.attention.self.key.bias\n",
      "text_encoder.transformer.encoder.layer.11.attention.self.value.weight\n",
      "text_encoder.transformer.encoder.layer.11.attention.self.value.bias\n",
      "text_encoder.transformer.encoder.layer.11.attention.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.11.attention.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "text_encoder.transformer.encoder.layer.11.intermediate.dense.weight\n",
      "text_encoder.transformer.encoder.layer.11.intermediate.dense.bias\n",
      "text_encoder.transformer.encoder.layer.11.output.dense.weight\n",
      "text_encoder.transformer.encoder.layer.11.output.dense.bias\n",
      "text_encoder.transformer.encoder.layer.11.output.LayerNorm.weight\n",
      "text_encoder.transformer.encoder.layer.11.output.LayerNorm.bias\n",
      "text_encoder.proj.0.weight\n",
      "text_encoder.proj.2.weight\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"outputs/exp-rank-8/vit/fewshot_10_percent/checkpoints/best_model/model.pth\", map_location=\"cpu\")\n",
    "\n",
    "# print the keys in the checkpoint\n",
    "for k in ckpt.keys():\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8dd3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_768in_512out\n",
      "w_a_000\n",
      "w_a_001\n",
      "w_a_002\n",
      "w_a_003\n",
      "w_a_004\n",
      "w_a_005\n",
      "w_a_006\n",
      "w_a_007\n",
      "w_a_008\n",
      "w_a_009\n",
      "w_a_010\n",
      "w_a_011\n",
      "w_a_012\n",
      "w_a_013\n",
      "w_a_014\n",
      "w_a_015\n",
      "w_a_016\n",
      "w_a_017\n",
      "w_a_018\n",
      "w_a_019\n",
      "w_a_020\n",
      "w_a_021\n",
      "w_a_022\n",
      "w_a_023\n",
      "w_b_000\n",
      "w_b_001\n",
      "w_b_002\n",
      "w_b_003\n",
      "w_b_004\n",
      "w_b_005\n",
      "w_b_006\n",
      "w_b_007\n",
      "w_b_008\n",
      "w_b_009\n",
      "w_b_010\n",
      "w_b_011\n",
      "w_b_012\n",
      "w_b_013\n",
      "w_b_014\n",
      "w_b_015\n",
      "w_b_016\n",
      "w_b_017\n",
      "w_b_018\n",
      "w_b_019\n",
      "w_b_020\n",
      "w_b_021\n",
      "w_b_022\n",
      "w_b_023\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open('outputs/exp-rank-8/vit/fewshot_10_percent/checkpoints/best_model/lora_weights.safetensors', framework=\"pt\", device=\"cpu\") as f:\n",
    "    for k in f.keys():\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2490db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RobustMedClip\n",
    "\n",
    "model = RobustMedClip("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"/home/raza.imam/Documents/rmedclip/data/MediMeta-C\",\n",
    "    repo_id=\"razaimam45/MediMeta-C\",\n",
    "\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b1662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
